{
  "hash": "08a46b65df65632598f5ea47e4d76af9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Web Scraping Publications from Google Scholar'\ndescription: 'Extract title, citation count and year of publications for an author from Google Scholar.'\nauthor: \n  - name: Purushottam Mohanty\ndate: \"2022-02-06\"\ncategories: [R]\n---\n\n\n\nThis post shows how to extract title (including co-authors and journal), citation count and year of publication for all available publications from an author profile in Google Scholar. \n\n\n\n\n\n\n\n## Extract List of Faculty Names\nFor this example, I use names of Stanford Computer Science faculty members .\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Stanford CS Faculty\nhtmlpage = read_html(\"https://cs.stanford.edu/directory/faculty\")\n\n# regular faculty\nfaculty = htmlpage %>%\n  html_elements(xpath = '//*[@id=\"node-113\"]/div/div[1]/div/div/table[1]') %>%\n  html_table() %>%\n  .[[1]] %>%\n  as.data.frame()\n\n# dataframe headers\nnames(faculty) = c(\"name\", \"phone\", \"office\", \"email_prefix\")\n```\n:::\n\n\n\n## Function to Get Faculty Publications from Google Scholar\nThe below function first searches the corresponding researcher's name in Google Scholar and obtains the Google Scholar Author ID for the same. Thereafter, it makes another request to obtain the list of publications from the profile page. Google Scholar uses pagination and only shows up the most cited 100 publications first. Another request is required to be made for the next 100 publications. Since, repeated requests can cause Google to temporarily block requests from an IP address or introduce a CAPTCHA, the function below makes page requests with a 3 second delay. Using this method, we extract the full title of the publication including the names of co-authors and the journal it was published at. Along with the title we also extract the year of publication and the total citation count of the publication.                           \n      \nFinally, it appends all the data from all the pages and outputs a dataframe. At the time of writing this post, the following method worked without any issue however your mileage may vary depending on how many requests you're making and at what time you're making them. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# function - get faculty publications with citations and year of publications\n# using google scholar search and google scholar author profiles\n# function takes one attribute - name of faculty\nget_faculty_pubs = function(x){ \n  \n  # tryCatch skips loops in case of an error (example, if the faculty has no Google Scholar profile)\n  return(tryCatch({\n    \n    # get faculty name\n    faculty_name = x\n    \n    # split name and tidy search string\n    search_string = paste(str_split(faculty_name, \" \")[[1]][1], \n                          str_split(faculty_name, \" \")[[1]][2], \n                          sep = \"+\")\n    \n    # gc author search url\n    gc_author_search_url = paste0(\n      'https://scholar.google.com/citations?view_op=search_authors&hl=en&mauthors=',\n      search_string, \"&btnG=\")\n    \n    # checks if author is available and gets its Google Scholar ID\n    # get author search ID\n    gc_author_id = read_html(gc_author_search_url) %>%\n      html_elements(xpath = '//*[@id=\"gsc_sa_ccl\"]/div/div/a') %>%\n      .[[1]] %>%\n      html_attr(\"href\")\n    \n    # gc uses pagination with a maximum return of 100 entries \n    # loop over each page (I loop it over 5 pages or 500 entries) \n    # (most faculty don't have greater than 400 publications or above 300 \n    #    there's repeated pubs or working papers without any citations or year)  \n    gc_author_pubs = lapply(seq(0, 300, 100), function(i){\n      \n      # google scholar author profile url\n      gc_author_url = paste0(\"https://scholar.google.com\", gc_author_id,\n                             \"&oi=ao\", \"&cstart=\", i, \"&pagesize=100\")\n      \n      # get html of author profile\n      gc_author_html = read_html(gc_author_url)\n      \n      # get all publications of the author\n      gc_author_pubs = gc_author_html %>%\n        html_elements(xpath = '//*[@id=\"gsc_a_t\"]') %>%\n        html_table() %>%\n        .[[1]]\n      \n      # fix column names\n      names(gc_author_pubs) = c(\"publication\", \"citation_count\", \"year\")\n      \n      # author affiliation for confirmation\n      gc_author_affil = gc_author_html %>%\n        html_elements(xpath = '//*[@id=\"gsc_prf_i\"]/div[2]/a') %>%\n        html_text()\n      \n      # add author affiliation\n      gc_author_df = gc_author_pubs %>%\n        mutate(author_affil = gc_author_affil)\n      \n      # add delay of 5 secs (to avoid Google detecting these requests)\n      date_time = Sys.time()\n      while((as.numeric(Sys.time()) - as.numeric(date_time))<3){}\n      \n      # get dataframe\n      return(gc_author_df)\n    })\n    \n    # append dataset from each page for the same author\n    bind_rows(gc_author_pubs) %>%\n      # remove unnecessary rows\n      filter(!year %in% \"Year\") %>% \n      # add faculty name\n      mutate(author = x) %>%\n      # drop error message (after the last publication this error message gets added)\n      filter(!year %in% \"There are no articles in this profile.\")\n    \n  }, error = function(e){NULL}))\n  \n}\n```\n:::\n\n\n\n## Get Publications for all Authors\nI use the author names that I extracted earlier to get all the publications for all the faculties. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# total number of faculty at Stanford with industry affiliation\nn_fac_with_ind_affil = NROW(faculty[faculty$industry_affil == 1, 'name'])\n\n# get publications for all such professors\ndf_final = lapply(1:n_fac_with_ind_affil, function(i){\n  # faculty name\n  fac_name = faculty[faculty$industry_affil == 1, 'name'][i]\n  # get publications data frame using function built earlier\n  get_faculty_pubs(fac_name)\n})\n\n# append all the data\ndf_final = bind_rows(df_final)\n```\n:::\n\n\n\nNote that some authors do not have a Google Scholar profile. In the case, the function simply outputs a NULL value.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}